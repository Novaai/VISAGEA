#Final Project File
import nltk
import cv2
from matplotlib import pyplot as plt
import pytesseract
import re
import mysql.connector
from datetime import datetime
from collections import defaultdict
#UI Addition
import tkinter as tk
from tkinter import filedialog, messagebox
import os
#AI Model Processing Integration
import pandas as pd
import joblib
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

# db setup
db_config = {
    'host': 'localhost',
    'user': 'root',
    'password': '',
    'database': 'visagea'
}

def load_model():
    MODEL_PATH = "balancedbagging_smote_randomforest_model.pkl"
    try:
        model = joblib.load(MODEL_PATH)
        print("Model loaded successfully.")
        return model
    except Exception as e:
        print(f"Error loading model: {e}")
        return None

def process_csv_with_model():
    model = load_model()
    if model is None:
        messagebox.showerror("Error", "Could not load model file.")
        return

    base_dir = os.getcwd()
    csv_path = filedialog.askopenfilename(title="Select CSV File", initialdir=base_dir,
                                          filetypes=[("CSV files", "*.csv")])
    if not csv_path:
        return

    df = pd.read_csv(csv_path)

    # Assuming your model pipeline includes preprocessing and only needs feature columns
    X = df.copy()  # You may want to drop target column or non-feature columns if present

    try:
        preds = model.predict(X)
        probs = model.predict_proba(X)[:, 1]  # positive class probability
    except Exception as e:
        messagebox.showerror("Prediction error", f"Error during prediction: {e}")
        return

    df["Predicted_CLASS"] = preds
    df["Predicted_PROBABILITY"] = probs

    save_path = filedialog.asksaveasfilename(title="Save CSV with Predictions", defaultextension=".csv",
                                             filetypes=[("CSV files", "*.csv")])
    if save_path:
        df.to_csv(save_path, index=False)
        messagebox.showinfo("Success", f"Predictions saved to:\n{save_path}")

## VISAGEA - Vision Inference and Sentiment Analysis for General Extraction and Automation ##
image_files = []
# test set 1
#r"images/Sample Data/scan_20241230092135.jpg",
#r"images/Sample Data/scan_20241230092137.jpg"
images = []

# image display
def display(im_path):
    dpi = 80
    im_data = plt.imread(im_path)
    height, width, depth = im_data.shape

    # figure sizing
    figsize = width / float(dpi), height / float(dpi)

    # creating figure of right size with one axes that takes up size of full figure
    fig = plt.figure(figsize=figsize)
    ax = fig.add_axes([0, 0, 1, 1])

    # Hide spines, ticks, etc.
    ax.axis('off')

    # Displaying the image
    ax.imshow(im_data, cmap='gray')
    plt.show()

# invert function
def invert_image(image):
    image = cv2.bitwise_not(image)
    return (image)

# binarization function
def grayscale(image):
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# noise removal fuction
def noise_removal(image):
    import numpy as np
    kernel = np.ones((1, 1), np.uint8)
    # dilation -> set to 1 iteration currently
    image = cv2.dilate(image, kernel, iterations=1)
    kernel = np.ones((1, 1), np.uint8)
    # erosion -> set to 1 iteration currently
    image = cv2.erode(image, kernel, iterations=1)
    image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)
    image = cv2.medianBlur(image, 3)
    return (image)

# dilation function (font thickener)
def thick_font(image):
    import numpy as np
    image = cv2.bitwise_not(image)
    kernel = np.ones((2, 2), np.uint8)
    image = cv2.dilate(image, kernel, iterations=1)
    image = cv2.bitwise_not(image)
    return (image)

# erosion function (font thinner)
def thin_font(image):
    import numpy as np
    image = cv2.bitwise_not(image)
    kernel = np.ones((2, 2), np.uint8)
    image = cv2.erode(image, kernel, iterations=1)
    image = cv2.bitwise_not(image)
    return (image)

def display_image(image, title):
    window_height = 750
    window_width = 650
    resized_image = cv2.resize(image, (window_width, window_height))
    cv2.imshow(title, resized_image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

# Calculate skew angle of an image
def getSkewAngle(cvImage) -> float:
    # Prep image, copy, convert to gray scale, blur, and threshold
    newImage = cvImage.copy()
    # gray = cv2.cvtColor(newImage, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(cvImage, (9, 9), 0)  # cvImage was gray
    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]

    # Apply dilate to merge text into meaningful lines/paragraphs.
    # Use larger kernel on X axis to merge characters into single line, cancelling out any spaces.
    # But use smaller kernel on Y axis to separate between different blocks of text
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (30, 5))
    dilate = cv2.dilate(thresh, kernel, iterations=5)

    # Find all contours
    contours, hierarchy = cv2.findContours(dilate, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    contours = sorted(contours, key=cv2.contourArea, reverse=True)

    # Find largest contour and surround in min area box
    largestContour = contours[0]
    minAreaRect = cv2.minAreaRect(largestContour)

    # Determine the angle. Convert it to the value that was originally used to obtain skewed image
    angle = minAreaRect[-1]
    if angle < -45:
        angle = 90 + angle
    return -1.0 * angle

# Rotate the image around its center
def rotateImage(cvImage, angle: float):
    newImage = cvImage.copy()
    (h, w) = newImage.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    newImage = cv2.warpAffine(newImage, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
    return newImage

# Deskew image
def deskew(cvImage):
    angle = getSkewAngle(cvImage)
    return rotateImage(cvImage, -1.0 * angle)

def remove_borders(img):
    contours, heirarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cntsSorted = sorted(contours, key=lambda x: cv2.contourArea(x))
    cnt = cntsSorted[-1]
    x, y, w, h = cv2.boundingRect(cnt)
    crop = img[y:y + h, x:x + w]
    return (crop)

def set_up_bounding_boxes(gray_image):
    blur = cv2.GaussianBlur(gray_image, (21, 21), 0)  # was 7,7
    display_image(blur, "Blurred image")
    # --->
    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]
    cv2.imwrite("temp/index_thresh.png", thresh)
    display_image(thresh, "Threshed image")
    # --->
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 13))
    cv2.imwrite("temp/index_kernel.png", kernel)
    display_image(kernel, "Kernel image")
    # --->
    dilate = cv2.dilate(thresh, kernel, iterations=1)
    cv2.imwrite("temp/index_dilate.png", dilate)
    display_image(dilate, "Dilated image")
    # --->
    # seting up bounding boxes needs fixing***
    # def draw_bounding_boxes(image, dilate):
    cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    cnts = sorted(cnts, key=lambda x: cv2.boundingRect(x)[0])  # highlight
    return cnts

def get_roi(cnts, img):
    for c in cnts:
        x, y, w, h = cv2.boundingRect(c)
        if h >= 0 and w >= 0:  # was 200,20 curr 25 <= h <= 70 and w >= 200
            roi = img[y:y + h, x:x + w]  # Region of interest for OCR #img
            cv2.imwrite("temp/roi.jpg",
                        roi)  # ---> Fix this. BBOXES TOO SMALL AND ORIENTED VERTICALLY WIDE BOXES INSTEAD OF HORIZONTALLY WIDE
            cv2.rectangle(img, (x, y), (x + w, y + h), (36, 255, 12), 2)  # img
    display_image(img, "Region of interest")
    return roi

def process_images(image_files):
    for idx, image_file in enumerate(image_files):
        img = cv2.imread(image_file)
        # View Original Image
        display_image(img, f"Original Image {idx + 1}")
        # Apply inversion
        inverted_image = invert_image(img)
        cv2.imwrite(f"temp/inverted_{idx + 1}.jpg", inverted_image)
        display_image(inverted_image, f"Inverted Image {idx + 1}")
        # Apply binarization
        gray_image = grayscale(img)
        cv2.imwrite(f"temp/gray_{idx + 1}.jpg", gray_image)
        display_image(gray_image, f"Gray Image {idx + 1}")
        thresh, im_bw = cv2.threshold(gray_image, 127, 255,cv2.THRESH_BINARY)  # Note: adjust first value only when fine tuning
        cv2.imwrite(f"temp/im_bw_{idx + 1}.jpg", im_bw)
        display_image(im_bw, f"Black and White {idx + 1}")
        # Apply noise removal
        noiseless_image = noise_removal(im_bw)
        cv2.imwrite(f"temp/noiseless_image_{idx + 1}.jpg", noiseless_image)
        display_image(noiseless_image, f"Noiseless Image {idx + 1}")
        # Apply border removal
        borderless_image = remove_borders(noiseless_image)
        cv2.imwrite(f"temp/borderless_image_{idx + 1}.jpg", borderless_image)
        display_image(borderless_image, f"Borderless Image {idx + 1}")
        # Apply deskewing on borderless image
        deskewed_image = deskew(borderless_image)
        cv2.imwrite(f"temp/deskewed_image_{idx + 1}.jpg", deskewed_image)
        display_image(deskewed_image, f"Deskewed Image {idx + 1}")
        # Apply font thickening #Optional
        dilated_image = thick_font(noiseless_image)
        cv2.imwrite(f"temp/dilated_image_{idx + 1}.jpg", dilated_image)
        display_image(dilated_image, f"Dilated Font Image {idx + 1}")
        # Apply font thinning #Optional
        eroded_image = thin_font(noiseless_image)
        cv2.imwrite(f"temp/eroded_image_{idx + 1}.jpg", eroded_image)
        display_image(eroded_image, f"Eroded Font Image {idx + 1}")
        # Acquire Contours
        cnts = set_up_bounding_boxes(gray_image)
        # Acquire Regions of Interest
        img2 = img.copy()  # creating (img2) a copy of img for roi to ovewrite
        roi = get_roi(cnts, img2)  # overwriting img2
        cv2.imwrite(f"temp/roi_{idx + 1}.jpg", roi)

#extracting and hanlding text
def process_paragraphs(ocr_result):
    # old
    # paragraphs = ocr_result.split('\n\n')
    # more thorough
    paragraphs = re.split(r'\n\s*\n+', ocr_result.strip())
    #par_collection = []  # Collecting paragraphs for further use
    temp_paragraph = ""
    case_type = ""
    acting_position = ""
    effect_date = ""
    prospect = ""

    # Process each paragraph
    for idx, paragraph in enumerate(paragraphs):
        if temp_paragraph and (temp_paragraph.endswith('and') or temp_paragraph.endswith(';')):
            paragraph = temp_paragraph + "\n" + paragraph
            temp_paragraph = ""
        else:
            temp_paragraph = paragraph

        ite_id = idx  # Use ite_id for the current iteration (one entry per ite_id)
        name_match = None
        ABSA_result = None

        # Perform sentiment analysis (optional, as per your existing code)
        ABSA_result = analyse_sentiment(paragraph, lex)  # Assuming analyse_sentiment is your sentiment analysis function

        # Create a dictionary for each row with initial empty columns
        row_data = {
            'ite_id': ite_id,
            'ABSA_result': [],
            'Full Name': [],
            'TS Number': [],
            #			'Full Name': None,
            'case_type': [],
            'effect_date': effect_date,
            'prospect': [],
            'Acting Position': acting_position,
            'Title': None,
            #			'First and Last Name': None,
            #			'TS Number': None,
            'NRC Number': [],
            'Job Title': None,
            'Salary Scale': None,
            'Name of School': None,
            'District': None,
            'Province': None,
            'Paragraph': [] #***
        }
        # conversion of absa dicionary insto string
        ABSA_str = str(ABSA_result)
        # Insertion of absa string to row_data ***
        row_data["ABSA_result"].append(ABSA_str)
        # Full Name Handling
        for match in re.finditer(name_pattern, paragraph):
            unique_names = {match.group() for match in re.finditer(name_pattern, paragraph)}
            for name in unique_names:
                row_data["Full Name"].append(name)
        # TS Number
        for match in re.finditer(TS_Number_pattern, paragraph):
            unique_TS_numbers = {match.group() for match in re.finditer(TS_Number_pattern, paragraph)}
            for ts_number in unique_TS_numbers:
                row_data["TS Number"].append(ts_number)
        # NRC Number
        for match in re.finditer(NRC_Number_pattern, paragraph):
            unique_NRC_numbers = {match.group() for match in re.finditer(NRC_Number_pattern, paragraph)}
            for nrc_number in unique_NRC_numbers:
                row_data["NRC Number"].append(nrc_number)
        # Case Type Handling
        if 'transferred' in paragraph.lower():
            case_type = "Transfer"
            row_data["case_type"].append(case_type)
        elif 'act as' in paragraph.lower():
            case_type = "Acting Appointment"
            row_data["case_type"].append(case_type)
        # prospect handling
        if 'substantive promotion' in paragraph.lower():
            prospect = "Substantive Promotion"
            row_data["prospect"].append(prospect)
        elif 'administrative convenience' in paragraph.lower():
            prospect = "Administrative Convenience"
            row_data["prospect"].append(prospect)

        # Apply regex patterns to extract data
        for pattern, label in patterns:
            matches = re.findall(pattern, paragraph)
            if matches:
                # Insert matches as separate columns for each ite_id
                match_index = 1
                for match in matches:
                    # Check if we have multiple matches and avoid column conflicts
                    column_name = f"{label}_{match_index}" if match_index > 1 else label
                    row_data[column_name] = match[0] if isinstance(match, tuple) else match
                    match_index += 1

        # Additional extraction rules for Acting Appointment and Transfer
        if 'act as' in paragraph.lower():
            case_type = "Acting Appointment"
            acting_match = re.search(
                r'(?i)(Head\s+Teacher|Deputy\s+Head\s+Teacher|Subject\s+Teacher|Class\s+Teacher|Head\s+of\s+Department\s+-\s+[^\s(]+(?:\s+[^\s(]+)*)',
                paragraph)
            if acting_match:
                acting_position = acting_match.group(0)
                row_data['Acting Position'] = acting_position

            # Extract school name after "at " and before " school"
            school_match = re.search(r'at\s+([^\s]+)\s+school', paragraph, re.IGNORECASE)
            if school_match:
                row_data['School Name'] = school_match.group(1)

            # Extract period of appointment
            period_match = re.search(r'period of \(\d{1}\)', paragraph)
            if period_match:
                row_data['Period of Appointment'] = period_match.group(0)

            # Extract effect date
            effect_date = extract_date(paragraph)
            if effect_date:
                row_data['Effect Date'] = effect_date

        if 'transferred' in paragraph.lower():
            case_type = "Transfer"
            school_match = re.findall(r'(\b\w+\b)\s+(\b\w+\b)\s+(\b\w+\b)\s+(?=School\b|school\b)', paragraph)
            if school_match:
                row_data['Transfer School Name'] = school_match[0]

            # Extract new position and effect date after "School "
            new_position_match = re.search(r'School\s+([^\s]+)\s+with', paragraph)
            if new_position_match:
                row_data['New Position'] = new_position_match.group(1)

            # Extract effect date (after "with")
            effect_date_match = re.search(r'with\s+([^\s]+)\s+(\w+)\.', paragraph)
            if effect_date_match:
                row_data['Effect Date'] = effect_date_match.group(0)

        # Add the current ite_id row with all matches and other data to identified_results
        identified_results.append(row_data)

        par_collection.append(paragraph)  # Collect paragraphs for later use
        #insertion of paragraph to row data
        row_data["Paragraph"].append(par_collection)
    # Printing the results for ite_id 9
    # ite_id_to_print = 9
    # for result in identified_results:
    #	if result['ite_id'] == ite_id_to_print:
    #		print("\n--- Identified Result for ite_id:", ite_id_to_print, "---")
    #		for key, value in result.items():
    #			print(f"{key}: {value}")
    #		print("-" * 30)

    return identified_results, par_collection

# Looking for text patterns for classification and extraction
def ocr_content(images):
    # Key:
    # _0 = Support Variable
    # _1 = HR_Name
    # _2 = HR_Title
    # _3 = File_Code
    processed = []
    pars_collection = []
    _0, _1, _2, _3 = None, None, None, None
    for idx, img in enumerate(images):
        ocr_result = pytesseract.image_to_string(img, lang='eng').strip()  # Perform OCR and remove whitespace
        # print(ocr_result)
        file_code_match = re.search(r'TSC\/\d{3}\/\d\/\d{1,2}', ocr_result, re.IGNORECASE)
        support_match = re.search(r'\b(I support|I do not support)\b', ocr_result, re.IGNORECASE)
        directive_match = re.search(r'consideration and directive\.', ocr_result, re.IGNORECASE)
        # Process the OCR result text
        processed, par_collection = process_paragraphs(ocr_result)
        pars_collection.append(par_collection)
        # for row in processed:
        # print(row)
        # must add a condition to stop adding to paragraphs when the word Supported is detected at the start of a paragraph***
        # Retrieve Support Status
        if file_code_match and _3 is None:
            _3 = file_code_match.group()
        # print(_3)
        if support_match and _0 is None:
            support_phrase = support_match.group().lower()
            _0 = "Supported" if "i support" in support_phrase else "Not Supported"
        # print(_0)
        # Retrieve  HR_Name and HR_Title
        if directive_match and _1 is None and _2 is None:
            # Extract everything after the directive match
            after_directive = ocr_result[directive_match.end():].strip()
            paragraphs = [p.strip() for p in re.split(r'\n\s*\n', after_directive) if p.strip()]

            if paragraphs:
                HR_Details = paragraphs[0]  # Use the first paragraph for HR details
                lines = HR_Details.split('\n', 1)  # Split into two lines
                if len(lines) >= 2:
                    _1, _2 = lines[0].strip(), lines[1].strip()
                # print(_1)
                # print(_2)
    return _0, _1, _2, _3, processed, pars_collection

# Regex Function
patterns = [
    (r'M(r\.|rs\.|s\.)', "Title"),  # Title
    (r'(?i)(Head\s+Teacher|Deputy\s+Head\s+Teacher|Subject\s+Teacher|Class\s+Teacher|Head\s+of\s+Department\s+-\s+[^\s(]+(?:\s+[^\s(]+)*)',
     "Job Title"),  # Job Title
    (r'\({1}[A-Z]{1}\){1}', "Salary Scale"),  # Salary Scale
    (r'(\b\w+\b)\s+(\b\w+\b)\s+(\b\w+\b)\s+(?=School\b|school\b)', "Name of School"),  # Name of School
    (r'(\b\w+\b)\s+(?=[Dd]istrict)', "District"),  # District
    (r'(\b\w+\b)\s+(?=[Pp]rovince)', "Province")  # Province
]
name_pattern = r'M(r\.|rs\.|s\.)\s\w+\s\w+'
TS_Number_pattern = r'\(TS\.\s\d+\)'
NRC_Number_pattern = r'\(NRC\/\d{6}\/\d{2}\/1\)'

# Lexicon for Aspect-Based Sentiment Analysis
lex = {
    "positive": {
        "terms": [
            {"word": "selected", "score": 0.8},
            {"word": "chosen", "score": 0.8},
            {"word": "appointed", "score": 0.85},
            {"word": "designated", "score": 0.75},
            {"word": "officially assigned", "score": 0.8},
            {"word": "commended", "score": 0.9},
            {"word": "i support", "score": 1.0},
            {"word": "promoted", "score": 0.9},
            {"word": "advanced", "score": 0.85},
            {"word": "elevated", "score": 0.9},
            {"word": "rewarded", "score": 0.9},
            {"word": "recognized", "score": 0.85},
            {"word": "honoured", "score": 0.9},
            {"word": "on post id", "score": 0.8},
            {"word": "assume duties of the higher post", "score": 0.8},
            {"word": "reassigned to a key position", "score": 0.85},
            {"word": "opportunity", "score": 0.9},
            {"word": "growth", "score": 0.85},
            {"word": "beneficial move", "score": 0.8},
            {"word": "strategic placement", "score": 0.9}
        ]
    },
    "negative": {
        "terms": [
            {"word": "overlooked", "score": -0.7},
            {"word": "rejected", "score": -0.8},
            {"word": "not considered", "score": -0.75},
            {"word": "delayed", "score": -0.6},
            {"word": "i do not support", "score": -2.0},
            {"word": "does not posess", "score": -0.9},
            {"word": "passed over", "score": -0.75},
            {"word": "denied", "score": -0.85},
            {"word": "blocked", "score": -0.9},
            {"word": "overlooked for advancement", "score": -0.8},
            {"word": "no post id", "score": -0.7},
            {"word": "demoted", "score": -0.9},
            {"word": "forced relocation", "score": -0.85},
            {"word": "adverse transfer", "score": -0.8},
            {"word": "compulsory reassignment", "score": -0.8},
            {"word": "loss of position", "score": -0.9},
            {"word": "contested move", "score": -0.75},
            {"word": "no treasury authority", "score": -1.0}
        ]
    },
    "neutral": {
        "terms": [
            {"word": "administrative convenience", "score": 0.0},
            {"word": "act", "score": 0.0},
            {"word": "when treasury authority is granted", "score": 0.0}
        ]
    }
}

#ABSA
def analyse_sentiment(temp_paragraph, lex):
    text = temp_paragraph.lower()  # Normalize text to lowercase to subvert case sen
    sentiment_scores = defaultdict(list)  # To store scores for positive, negative, and neutral terms

    # Iterate through sentiment types in the lexicon
    for sentiment_type, data in lex.items():
        for term in data["terms"]:
            word, score = term["word"], term["score"]
            if re.search(r'\b' + re.escape(word) + r'\b', text):  # Match word as whole word
                sentiment_scores[sentiment_type].append(score)

    # Aggregate sentiment scores
    result = {}
    for sentiment_type, scores in sentiment_scores.items():
        if scores:
            avg_score = sum(scores) / len(scores)
            result[sentiment_type] = {
                "average_score": round(avg_score, 2),
                "count": len(scores)  # Number of matches
            }

    # Overall sentiment
    overall_score = sum(score for scores in sentiment_scores.values() for score in scores)
    total_matches = sum(len(scores) for scores in sentiment_scores.values())
    overall_avg_score = overall_score / total_matches if total_matches > 0 else 0

    overall_sentiment = (
        "positive" if overall_avg_score > 0 else
        "negative" if overall_avg_score < 0 else
        "neutral"
    )

    result["overall"] = {
        "average_score": round(overall_avg_score, 2),
        "sentiment": overall_sentiment
    }

    return result

# Date Extraction
def extract_date(text):
    date_match = re.search(r'(\d{1,2}[a-z]{2}\s+[A-Za-z]+,\s+\d{4}|the date)', text)
    if date_match:
        return date_match.group(0) if date_match.group(0) != 'the date' else 'Duty Assumption Date'
    return None

# Start the extraction
process_images(image_files)  # The big red button
results = []

identified_results = []
par_collection = []  # Collecting paragraphs for further use

#defining final function
def final_processing(images):
    # Fetch values through call.
    _0, _1, _2, _3, processed, pars_collection = ocr_content(images)  # Pass the list of images to the function
    # trimming empty results
    identified_results = [row for row in processed if row['TS Number'] not in [None, '', []]]

    for result in identified_results:
        print("\n--- Identified Result for ite_id:", result.get('ite_id', 'N/A'), "---")
        for key, value in result.items():
            print(f"{key}: {value}")
            print("-" * 30)
        print("<------------>")

    for row in processed:
        print(row)

    # Append the final result to the results list
    results.append({
        "File_Code": _3,
        "support_var": _0,
        "HR_Name": _1,
        "HR_Title": _2
    })

    # Process and print results
    for result in results:
        print(
            f"File Code: {result['File_Code']}, Support: {result['support_var']}, HR Name: {result['HR_Name']}, HR Title: {result['HR_Title']}")

    # Finalise with DB insertion

    try:
        # Connect to the database
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()

        # Insert query
        insert_query = """
        INSERT INTO doc_scans_ (
            file_code, doc_author, doc_author_pos, doc_support_status, case_type, 
            case_name_1, case_name_2, case_ts_no_1, case_ts_no_2, 
            case_nrc_no_1, case_nrc_no_2, case_job_title_1, case_job_title_2, 
            case_salary_scale_1, case_salary_scale_2, case_school_1, case_school_2, 
            case_district_1, case_district_2, case_province_1, case_province_2, 
            aa_or_tfer_type, case_aa_period, case_date, Sentiment, Paragraph
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """

        # Data to insert
        doc_author = _1
        file_code = _3
        doc_author_pos = _2
        doc_support_status = _0
        #	case_aa_type = "Type X"  # Optional
        case_aa_period = 3  # Optional
        case_date = "To be resolved"  # Optional

        for identified_result in identified_results:
            # Access the value for the 'case_type' key directly from the dictionary
            Sentiment = identified_result.get('ABSA_result', '')
            case_name_1 = identified_result.get('Full Name', [''])[0]
            case_name_2 = "Feature in development"
            case_type = identified_result.get('case_type', [''])[0]
            case_ts_no_1 = identified_result.get('TS Number', [''])[0]
            case_ts_no_2 = identified_result.get('TS Number_2', [''])[0]
            case_school_1 = identified_result.get('Name of School', '')  #
            case_nrc_no_1 = "Feature not added yet"
            case_nrc_no_2 = "Feature not added yet"
            # case_nrc_no_1 = identified_result.get('NRC Number', [''])[0]
            # case_nrc_no_2 = identified_result.get('NRC Number_2', [''])[0]
            case_job_title_1 = identified_result.get('Job Title', '')
            case_job_title_2 = identified_result.get('Acting Position', '')
            case_salary_scale_1 = identified_result.get('Salary Scale', '')
            case_salary_scale_2 = identified_result.get('Salary Scale_2', '')
            case_school_2 = identified_result.get('Transfer School Name', [''])[0]
            case_district_1 = identified_result.get('District', '')  #
            case_district_2 = identified_result.get('District_2', [''])[0]
            case_province_1 = identified_result.get('Province', '')  #
            case_province_2 = identified_result.get('Province_2', [''])[0]
            aa_or_tfer_type = identified_result.get('prospect', '')
            effect_date = identified_result.get('effect_date', '')
            Paragraph = identified_result.get('Paragraph', [''])[0]

            values = (
                file_code, doc_author, doc_author_pos, doc_support_status, case_type,
                case_name_1, case_name_2, case_ts_no_1, case_ts_no_2,
                case_nrc_no_1, case_nrc_no_2, case_job_title_1, case_job_title_2,
                case_salary_scale_1, case_salary_scale_2, case_school_1, case_school_2,
                case_district_1, case_district_2, case_province_1, case_province_2,
                aa_or_tfer_type, case_aa_period, case_date, Sentiment, Paragraph
            )
            values = tuple(', '.join(v) if isinstance(v, list) else v for v in values)

            cursor.execute(insert_query, values)

        # Commit the transaction
        conn.commit()

        print(f"Data inserted successfully. ID: {cursor.lastrowid}")

    except mysql.connector.Error as err:
        print(f"Error: {err}")

    finally:
        if conn.is_connected():
            cursor.close()
            conn.close()

def select_images():
    global image_files, images

    start_dir = os.path.join(os.getcwd(), "images", "Sample Data")
    file_paths = filedialog.askopenfilenames(
        title="Select Image Files",
        initialdir=start_dir,
        filetypes=[("Image Files", "*.jpg *.jpeg *.png *.bmp *.tif *.tiff")]
    )

    if file_paths:
        file_list.delete(0, tk.END)
        for path in file_paths:
            file_list.insert(tk.END, path)

        image_files = list(file_paths)
        images = [cv2.imread(path) for path in image_files]

        process_button.config(state=tk.NORMAL)
        status_label.config(text="Ready to process", fg="green")
    else:
        image_files = []
        images = []
        file_list.delete(0, tk.END)
        process_button.config(state=tk.DISABLED)
        status_label.config(text="No images selected", fg="red")

def trigger_processing():
    if images:
        status_label.config(text="Processing...", fg="blue")
        status_label.update_idletasks()
        #OPTIONAL - visually process images
        #process_images(image_files)
        #ocr and sentiment analyse + prepare db insert and insert data
        final_processing(images)  # <-- Your function call here
        status_label.config(text="Processing completed", fg="green")
    else:
        messagebox.showwarning("No Images", "Please select images first.")

def image_batch_ui():
    global file_list, process_button, status_label

    app = tk.Tk()
    app.title("Image Batch Selector")
    app.geometry("650x450")  # Slightly bigger window

    label = tk.Label(app, text="Select multiple image files to load:", font=("Arial", 12))
    label.pack(pady=10)

    select_button = tk.Button(app, text="Browse Files", command=select_images, font=("Arial", 10))
    select_button.pack(pady=5)

    file_list = tk.Listbox(app, width=70, height=10)
    file_list.pack(pady=10)

    status_label = tk.Label(app, text="No images selected", fg="red", font=("Arial", 10, "bold"))
    status_label.pack(pady=5)

    process_button = tk.Button(app, text="Process Document(s)", state=tk.DISABLED,
                               command=trigger_processing, font=("Arial", 11, "bold"), bg="#d3d3d3")
    process_button.pack(pady=10)

    close_button = tk.Button(app, text="Close", command=app.destroy, font=("Arial", 10))
    close_button.pack(pady=5)

    app.mainloop()

def csv_processing_ui():
    app = tk.Tk()
    app.title("CSV Model Prediction")
    app.geometry("400x150")

    label = tk.Label(app, text="Load a CSV file to predict with model", font=("Arial", 12))
    label.pack(pady=20)

    button = tk.Button(app, text="Select CSV & Predict", command=process_csv_with_model, font=("Arial", 12))
    button.pack(pady=10)

    close_button = tk.Button(app, text="Close", command=app.destroy, font=("Arial", 10))
    close_button.pack(pady=5)

    app.mainloop()

def main_launcher_ui():
    app = tk.Tk()
    app.title("Main Launcher")
    app.geometry("300x200")

    label = tk.Label(app, text="Choose an option:", font=("Arial", 14))
    label.pack(pady=15)

    def open_image_ui():
        app.destroy()
        image_batch_ui()

    def open_csv_ui():
        app.destroy()
        csv_processing_ui()

    btn_image = tk.Button(app, text="Process Image Batch", command=open_image_ui, width=20, font=("Arial", 12))
    btn_image.pack(pady=5)

    btn_csv = tk.Button(app, text="Process Records (CSV)", command=open_csv_ui, width=20, font=("Arial", 12))
    btn_csv.pack(pady=5)

    btn_close = tk.Button(app, text="Close", command=app.destroy, width=20, font=("Arial", 12))
    btn_close.pack(pady=5)

    app.mainloop()

if __name__ == "__main__":
    main_launcher_ui()